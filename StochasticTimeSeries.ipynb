{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9i7FCYGKYAxpLbkRBqywC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robcovino/IntroBiomolecularSimulations/blob/main/StochasticTimeSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0EBzYA3PbkI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-notebook')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Time series to probability distribution and thermodynamic equilibrium\n",
        "\n",
        "\n",
        "Let's look at the following time series $x(t)$ that was pre-generated and you can just load for your analysis.\n",
        "\n",
        "First, you can download the file at this link:\n",
        "\n",
        "https://hessenbox-a10.rz.uni-frankfurt.de/getlink/fiBYiYwdxF8JxniTMqgLJj/time_series.csv\n",
        "\n",
        "\n",
        "## Using Google Drive to store simulation data\n",
        "\n",
        "Google Colab does not allow users to keep data on their computing nodes. However, we can use Google Drive to read, write, and store our simulations files. Therefore, we suggest to you to:\n",
        "\n",
        "1. Create a folder in your own Google Drive and copy the necessary input files there.\n",
        "2. Copy the path of your created directory. We will use it below."
      ],
      "metadata": {
        "id": "fKyYkvygPliv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **Import Google Drive**\n",
        "#@markdown Click in the \"Run\" buttom to make your Google Drive accessible.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "ViWgRD-TXu_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **Load data into a local numpy variable**\n",
        "\n",
        "x = np.genfromtxt(\"drive/MyDrive/IntroBS/time_series.csv\")"
      ],
      "metadata": {
        "id": "Kek8qouWSCYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: What does the variable contain? What is the format of the data? How are the data organized? How many data points are there?\n",
        "\n",
        "tip: use len(x) or x.shape"
      ],
      "metadata": {
        "id": "BQGIzJkNa3ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x\n",
        "\n"
      ],
      "metadata": {
        "id": "f4wAk7M_Y0yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now look at the dynamic process that is described by the time series. The easiest way is to plot the first K values. Assuming that the samples in the TS were saved with a frequency $\\Delta t$, it means that considering the first K values corresponds to looking at the TS on a time interval $K \\Delta t$."
      ],
      "metadata": {
        "id": "2ci25qoub6zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **Plot first K values of the TS**\n",
        "\n",
        "K = 10\n",
        "\n"
      ],
      "metadata": {
        "id": "jjRjFpbVZpXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Inspect the TS at increasing values of K. \n",
        "1. What do you notice? \n",
        "2. What are the typical scales of the process? \n",
        "3. Can you discern any regularities? \n",
        "4. What quantities would you introduce to describe the process? \n",
        "5. What are the challenges you identify to describe the process? \n",
        "6. Can you ever make any general statement about the process? Or do you always need to see more data? "
      ],
      "metadata": {
        "id": "9qdZIvN2dk0b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ND4OPOErcg5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculating the average (mean) as a function of time**\n",
        "\n",
        "Q: Write a function that plots the average of x as a function of t instead of x \n",
        "itself. The function should associate each K to the average of x up until index K.\n",
        "\n",
        "Tip: a = np.zeros(N) makes an array of zeros long N, which you can use to store values in a for loop."
      ],
      "metadata": {
        "id": "mNVU_eO3et6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_ts_1(x, K):\n",
        "\n"
      ],
      "metadata": {
        "id": "_VMbbwG-fXqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNqahX8SflBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: How can you use this new tool to answer some of the questions of the previous point?\n",
        "\n",
        "## Having access to sampling of different time scales\n",
        "\n",
        "Q: Now repeat the same analysis you did before, assuming that all the data you got are y = x[:6000]. Repeat the analysis you performed so far. Would your general methodological conclusions change? Would you be able to make the same statement about the particular system?"
      ],
      "metadata": {
        "id": "EE98VcLHi_1_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85BbSZwJgdGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From time series to probability distributions\n",
        "\n",
        "Let's continue considering the case where all the data we have are y = x[:6000]. We peformed the analysis above and are now convinced that our time series is *stationary* and want to make switch to a probabilistic description. \n",
        "\n",
        "The simplest way of doing that is to calculate a *histogram* from our time series. This corresponds to estimating probabilities as frequencies of a stationary process. It comprises the following steps:\n",
        "\n",
        "1. Bin your variable, ie, coarse-grain your data into a discrete set of small intervals (bins or microstates). \n",
        "2. Count how often your time series is in a given bin.\n",
        "3. Plot a bar for each bin. The height of the bin should be rescaled, such that the total area described the all bars is 1 (probability must be normalized). \n",
        "\n",
        "Q: Write a function that makes a histogram of y (Note: there are functions that do that for you, but it's good to write your own function to appreciate each steps)."
      ],
      "metadata": {
        "id": "wi6NM0W2lo_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def histogram_a(data, n_bins):\n",
        "  "
      ],
      "metadata": {
        "id": "0oy8LBe8qsVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExT9VbOKrMih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Now let's use the new histogram tool to investigate the preivous time series. What are the pros and cons of this new way of looking at our data?\n",
        "\n",
        "Q: What statement can we make about the data contained in y?\n",
        "\n"
      ],
      "metadata": {
        "id": "-850XFWnrpdz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_bAeFXd-rVA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go back to the original data set x and investigate a histogram at increasing number of samples. \n",
        "\n",
        "Q: When are we actually allowed to interpret the histogram as an estimate of probabilities? \n",
        "\n",
        "Q: What guarantees us that x is actually y?\n",
        "\n",
        "## Coarse-graining of our data\n",
        "\n",
        "Now that you have a deep understanding of your data, you might agree that to make many of the statements and considerations that we discussed so far we do not need to take into account all details contained in x, ie, we do not need to look at the data with the highest resolution. We can coarse-grain the data, ie, look at the data with lower resolution, and get out the same sort of understanding.\n",
        "\n",
        "Q: How would you coarse-grain your data and why? Can you identify some (macro)states?\n",
        "\n",
        "Q: Use the plt.hist function to plot a histogram of your coarse grained data. Tip: Use a custom version of bins."
      ],
      "metadata": {
        "id": "qbjNWDAdsXWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SqeZNd7Yrg_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "emFWZf2Ntycq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oYJmHOHNudoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Use the previous tools to write a function that calculates the relative probability of 2 states. When does this number not depend on time any more? What must the underlying time series look like for this condition to be satisfied?"
      ],
      "metadata": {
        "id": "Seq6MXHCvjaO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "093381Ehu-3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aq3qB5W1wTBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "acGfkxt1wZgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qxbjWtZQxagr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time average, ensemble averages, and ergodicity\n",
        "\n",
        "So far we took averages in time. But now you have calculated probabilities associated to the dynamical process, and when we have probabilities, we calculate averages in a very different way, called ensemble average:\n",
        "\n",
        "$<x> = ∑_ip_i x_i$\n",
        "\n",
        "Q: Introduce discrete states $x_i$ (eg bins), estimate the associated probabilities $p_i$, and show that the average in time and the ensemble averages are the same."
      ],
      "metadata": {
        "id": "kJMDiF25p3CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# time average\n",
        "\n",
        "\n",
        "\n",
        "# ensemble average\n"
      ],
      "metadata": {
        "id": "mRUz8KVzyJMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNEAKwbt1uM5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}